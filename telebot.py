import openai
from pyrogram import Client, filters
from pyrogram.types import ReplyKeyboardMarkup, KeyboardButton
import requests
import os
from datetime import datetime, timedelta
from dotenv import load_dotenv
import json
import asyncio
import tiktoken

# Load environment variables from .env file
load_dotenv()

# Get credentials from environment variables
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
API_ID = int(os.getenv('API_ID'))
API_HASH = os.getenv('API_HASH')
PIPEDRIVE_API_TOKEN = os.getenv('PIPEDRIVE_API_TOKEN')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# Initialize Pyrogram Client
app = Client(
    "my_bot",
    bot_token=TELEGRAM_BOT_TOKEN,
    api_id=API_ID,
    api_hash=API_HASH
)

BASE_URL = 'https://api.pipedrive.com/v1/'

# Set up OpenAI API
openai.api_key = OPENAI_API_KEY

# Define supported languages with flags
languages = {
    "English": "üá¨üáß",
    "–†—É—Å—Å–∫–∏–π": "üá∑üá∫",
    "Espa√±ol": "üá™üá∏",
    "Fran√ßais": "üá´üá∑",
    "Deutsch": "üá©üá™",
    "‰∏≠Êñá": "üá®üá≥",
    "Êó•Êú¨Ë™û": "üáØüáµ",
    "ÌïúÍµ≠Ïñ¥": "üá∞üá∑",
    "Portugu√™s": "üáµüáπ",
    "Italiano": "üáÆüáπ",
    # Add more languages and flags as needed
}

# Define prompts in multiple languages
general_prompts = {
    "English": {
        "who are you": "I am CAI, a helpful assistant for Luxury World Key Concierge and Enlighted Minds.",
        "what is your goal": "My goal is to assist Luxury World Key Concierge and Enlighted Minds in providing excellent services and insights based on business data.",
        "what can we do to earn more money": "To earn more money, consider optimizing your sales funnel, improving customer service, and exploring new markets. Detailed strategies can be provided based on specific business data.",
        "which data is most often not filled in pipedrive": "The most frequently unfilled data in Pipedrive include contact details, follow-up activities, and deal values. Ensuring these fields are consistently filled can improve data quality.",
        "what are our strengths and weaknesses": "Your strengths include a dedicated team, strong client relationships, and a diversified service portfolio. Weaknesses may include inconsistent data entry and areas for process improvement.",
        "how do you help as an ai in concierge": "As an AI in concierge, I assist by managing and analyzing data, providing insights, optimizing processes, and enhancing customer service through personalized recommendations.",
        "how much money does lwk concierge make": "To determine the revenue of Luxury World Key Concierge, we need to analyze the financial data and sales figures recorded in our system."
    },
    "–†—É—Å—Å–∫–∏–π": {
        "who are you": "–Ø CAI, –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è Luxury World Key Concierge –∏ Enlighted Minds.",
        "what is your goal": "–ú–æ—è —Ü–µ–ª—å - –ø–æ–º–æ—á—å Luxury World Key Concierge –∏ Enlighted Minds –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –æ—Ç–ª–∏—á–Ω—ã–µ —É—Å–ª—É–≥–∏ –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∏–∑–Ω–µ—Å-–¥–∞–Ω–Ω—ã—Ö.",
        "what can we do to earn more money": "–ß—Ç–æ–±—ã –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –±–æ–ª—å—à–µ –¥–µ–Ω–µ–≥, —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–æ—Ä–æ–Ω–∫–∏ –ø—Ä–æ–¥–∞–∂, —É–ª—É—á—à–µ–Ω–∏—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –∏–∑—É—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Ä—ã–Ω–∫–æ–≤. –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-–¥–∞–Ω–Ω—ã—Ö.",
        "which data is most often not filled in pipedrive": "–ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –Ω–µ –∑–∞–ø–æ–ª–Ω—è–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ Pipedrive –≤–∫–ª—é—á–∞—é—Ç –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –∏ –∑–Ω–∞—á–µ–Ω–∏—è —Å–¥–µ–ª–æ–∫. –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –ø–æ–ª–µ–π –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö.",
        "what are our strengths and weaknesses": "–í–∞—à–∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –≤–∫–ª—é—á–∞—é—Ç –ø—Ä–µ–¥–∞–Ω–Ω—É—é –∫–æ–º–∞–Ω–¥—É, –∫—Ä–µ–ø–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏ –∏ –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å —É—Å–ª—É–≥. –°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –º–æ–≥—É—Ç –≤–∫–ª—é—á–∞—Ç—å –Ω–µ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.",
        "how do you help as an ai in concierge": "–ö–∞–∫ –ò–ò –≤ –∫–æ–Ω—Å—å–µ—Ä–∂-—Å–ª—É–∂–±–µ, —è –ø–æ–º–æ–≥–∞—é —É–ø—Ä–∞–≤–ª—è—Ç—å –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã –∏ —É–ª—É—á—à–∞—Ç—å –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.",
        "how much money does lwk concierge make": "–ß—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–æ—Ö–æ–¥—ã Luxury World Key Concierge, –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø—Ä–æ–¥–∞–∂, –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤ –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ."
    },
    # Add more translations as needed
}

# Function to fetch Pipedrive data
def get_pipedrive_data(endpoint, params=None):
    url = f"{BASE_URL}{endpoint}?api_token={PIPEDRIVE_API_TOKEN}"
    if params:
        for key, value in params.items():
            url += f"&{key}={value}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.json().get('data', [])
    else:
        print(f"Error fetching data from Pipedrive: {response.status_code}, {response.text}")
        return None

def get_user_details(user_id):
    endpoint = f'users/{user_id}'
    user_data = get_pipedrive_data(endpoint)
    if user_data:
        return user_data.get('name', f'User {user_id}')
    return f'User {user_id}'

def replace_ids_with_names(data):
    if isinstance(data, list):
        for item in data:
            if 'owner_id' in item:
                item['owner_name'] = get_user_details(item['owner_id'])
    return data

def get_pipelines():
    return get_pipedrive_data('pipelines')

def get_stages_by_pipeline(pipeline_id):
    params = {'pipeline_id': pipeline_id}
    return get_pipedrive_data('stages', params)

def get_deals_by_pipeline(pipeline_id):
    endpoint = f'pipelines/{pipeline_id}/deals'
    return replace_ids_with_names(get_pipedrive_data(endpoint))

def get_activities_by_pipeline(pipeline_id):
    params = {'pipeline_id': pipeline_id}
    return replace_ids_with_names(get_pipedrive_data('activities', params))

def get_deal_details(deal_id):
    endpoint = f'deals/{deal_id}'
    return replace_ids_with_names(get_pipedrive_data(endpoint))

def get_all_leads():
    return replace_ids_with_names(get_pipedrive_data('leads'))

def count_tokens(text, encoding_name='cl100k_base'):
    encoding = tiktoken.get_encoding(encoding_name)
    tokens = encoding.encode(text)
    return len(tokens)

def split_into_dynamic_chunks(data, max_tokens_per_chunk):
    """Split data into dynamically sized chunks based on the max tokens per chunk."""
    total_tokens = sum(count_tokens(json.dumps(item)) for item in data)
    num_chunks = max(1, (total_tokens + max_tokens_per_chunk - 1) // max_tokens_per_chunk)
    chunk_size = len(data) // num_chunks
    chunks = []

    for i in range(num_chunks):
        start_index = i * chunk_size
        end_index = (i + 1) * chunk_size if i != num_chunks - 1 else len(data)
        chunks.append(data[start_index:end_index])

    return chunks

async def process_chunk_with_ai(chunk, data_type, context):
    prompt = f"Here is the current conversation context:\n{context}\n\nProcess the following {data_type} data and provide a clear and concise summary:\n{json.dumps(chunk)}"
    try:
        response = await openai.ChatCompletion.acreate(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are CAI, a helpful assistant for Luxury World Key Concierge and Enlighted Minds. Provide clear, concise, and informative summaries of the provided data. If asked questions about leads or deals, refer to the provided data."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800
        )
        return response['choices'][0]['message']['content'].strip()
    except openai.error.OpenAIError as e:
        return f"An error occurred: {e}"

async def process_with_ai(data_chunks, data_type, context):
    summaries = await asyncio.gather(*[process_chunk_with_ai(chunk, data_type, context) for chunk in data_chunks])
    
    # Combine all summaries into one final prompt
    final_prompt = f"Here is the current conversation context:\n{context}\n\nHere are the summaries of each chunk of data. Please provide a final, concise summary of all the information:\n"
    final_prompt += "\n".join(summaries)
    
    try:
        final_response = await openai.ChatCompletion.acreate(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are CAI, a helpful assistant for Luxury World Key Concierge and Enlighted Minds. Provide clear, concise, and informative summaries of the provided data. If asked questions about leads or deals, refer to the provided data."},
                {"role": "user", "content": final_prompt}
            ],
            max_tokens=800
        )
        return final_response['choices'][0]['message']['content'].strip()
    except openai.error.OpenAIError as e:
        return f"An error occurred during final summary generation: {e}"

async def fetch_data_with_embeddings(query):
    # Generate embedding for the query
    query_embedding = openai.Embedding.create(
        input=query,
        model="text-embedding-ada-002"
    )["data"][0]["embedding"]

    # Fetch all relevant data from Pipedrive
    all_leads = get_all_leads()
    all_deals = get_pipedrive_data('deals')
    all_activities = get_pipedrive_data('activities')

    # Generate embeddings for all the data
    leads_embeddings = openai.Embedding.create(
        input=[json.dumps(lead) for lead in all_leads],
        model="text-embedding-ada-002"
    )["data"]
    
    deals_embeddings = openai.Embedding.create(
        input=[json.dumps(deal) for deal in all_deals],
        model="text-embedding-ada-002"
    )["data"]

    activities_embeddings = openai.Embedding.create(
        input=[json.dumps(activity) for activity in all_activities],
        model="text-embedding-ada-002"
    )["data"]

    # Find the most relevant leads, deals, and activities based on embeddings
    relevant_leads = []
    relevant_deals = []
    relevant_activities = []

    for lead, lead_embedding in zip(all_leads, leads_embeddings):
        similarity = cosine_similarity(query_embedding, lead_embedding["embedding"])
        if similarity > 0.85:  # Adjust threshold as needed
            relevant_leads.append(lead)

    for deal, deal_embedding in zip(all_deals, deals_embeddings):
        similarity = cosine_similarity(query_embedding, deal_embedding["embedding"])
        if similarity > 0.85:  # Adjust threshold as needed
            relevant_deals.append(deal)

    for activity, activity_embedding in zip(all_activities, activities_embeddings):
        similarity = cosine_similarity(query_embedding, activity_embedding["embedding"])
        if similarity > 0.85:  # Adjust threshold as needed
            relevant_activities.append(activity)

    return relevant_leads, relevant_deals, relevant_activities

def cosine_similarity(vec1, vec2):
    return sum(a * b for a, b in zip(vec1, vec2)) / (sum(a * a for a in vec1) ** 0.5 * sum(b * b for b in vec2) ** 0.5)

async def analyze_question_and_get_data(question, context, language):
    # General instructions and common questions
    instructions = """
    You are CAI, a helpful assistant for Luxury World Key Concierge and Enlighted Minds. You assist by managing and analyzing data, providing insights, optimizing processes, and enhancing customer service through personalized recommendations. Your goal is to assist Luxury World Key Concierge and Enlighted Minds in providing excellent services and insights based on business data.
    If asked questions about leads or deals, refer to the provided data.
    If asked questions that require financial analysis or data not directly available in Pipedrive, use your knowledge and any available data to provide a detailed and informative response.
    """

    # Check for general questions first
    question_lower = question.lower()
    if question_lower in general_prompts[language]:
        return general_prompts[language][question_lower]

    # Check for questions requiring data analysis
    if any(keyword in question_lower for keyword in ["lead", "deal", "activity", "money", "revenue", "income"]):
        relevant_leads, relevant_deals, relevant_activities = await fetch_data_with_embeddings(question)
        response = "Based on our analysis, here are the relevant data:\n\n"
        if relevant_leads:
            response += f"Leads:\n{json.dumps(relevant_leads, indent=2)}\n\n"
        if relevant_deals:
            response += f"Deals:\n{json.dumps(relevant_deals, indent=2)}\n\n"
        if relevant_activities:
            response += f"Activities:\n{json.dumps(relevant_activities, indent=2)}\n\n"
        if not (relevant_leads or relevant_deals or relevant_activities):
            response = "I couldn't find relevant data based on your query. Could you please specify more details?"
        return response

    # Combine the context with the question for processing
    final_prompt = f"Here is the current conversation context:\n{context}\n\nUser's question: {question}\n\nProvide a detailed and informative response based on the given data and context."
    
    try:
        final_response = await openai.ChatCompletion.acreate(
            model="gpt-4",
            messages=[
                {"role": "system", "content": instructions},
                {"role": "user", "content": final_prompt}
            ],
            max_tokens=800
        )
        return final_response['choices'][0]['message']['content'].strip()
    except openai.error.OpenAIError as e:
        return f"An error occurred during response generation: {e}"

async def send_processing_notification(client, message, data_type, data_count):
    await client.send_message(
        chat_id=message.chat.id,
        text=f"The answer will be sent in 1-3 minutes. Processing {data_count} {data_type}."
    )

# Dictionary to store user data
user_data = {}

# Command handler for /start
@app.on_message(filters.command("start"))
async def start(client, message):
    keyboard = ReplyKeyboardMarkup([
        [KeyboardButton("üìã Leads"), KeyboardButton("üíº Deals")],
        [KeyboardButton("üóìÔ∏è Activities")],
        [KeyboardButton("üß† Ask CAI"), KeyboardButton("üåê Change Language")]
    ], resize_keyboard=True)
    await message.reply("Welcome! Choose an option:", reply_markup=keyboard)

# Command handler for language change
async def change_language(client, message):
    language_buttons = [[KeyboardButton(f"{flag} {lang}")] for lang, flag in languages.items()]
    keyboard = ReplyKeyboardMarkup(language_buttons + [[KeyboardButton("üîô Back")]], resize_keyboard=True)
    await message.reply("Please select your language:", reply_markup=keyboard)

# Message handler for main menu buttons
@app.on_message(filters.text)
async def handle_buttons(client, message):
    text = message.text
    user_id = message.from_user.id

    if user_id not in user_data:
        user_data[user_id] = {"context": [], "language": "English"}

    if 'context' not in user_data[user_id]:
        user_data[user_id]['context'] = []

    if 'language' not in user_data[user_id]:
        user_data[user_id]['language'] = "English"

    if text == "üìã Leads":
        leads = get_all_leads()
        if not leads:
            await message.reply(f"No leads found.", reply_markup=ReplyKeyboardMarkup([[KeyboardButton("üîô Back")]], resize_keyboard=True))
        else:
            await send_processing_notification(client, message, "leads", len(leads))
            leads_chunks = split_into_dynamic_chunks(leads, 7000)  # Use dynamic chunking
            leads_summary = await process_with_ai(leads_chunks, "leads", user_data[user_id]['context'])
            await message.reply(f"Leads Summary:\n{leads_summary}", reply_markup=ReplyKeyboardMarkup([[KeyboardButton("üîô Back")]], resize_keyboard=True))
            user_data[user_id]['context'].append(f"Leads Summary:\n{leads_summary}")
        user_data[user_id]['current_module'] = "Leads"
    elif text == "üíº Deals" or text == "üóìÔ∏è Activities":
        pipelines = get_pipelines()
        pipeline_buttons = [[KeyboardButton(f"{pipeline['name']}")] for pipeline in pipelines]
        keyboard = ReplyKeyboardMarkup(pipeline_buttons + [[KeyboardButton("üîô Back")]], resize_keyboard=True)
        await message.reply(f"Choose a pipeline for {text[2:].lower()}:", reply_markup=keyboard)
        user_data[user_id]['current_module'] = text
    elif text == "üß† Ask CAI":
        await message.reply("Please type your question for CAI:")
        user_data[user_id]['current_module'] = "Ask CAI"
    elif text == "üåê Change Language":
        await change_language(client, message)
    elif text.startswith("üîô Back"):
        keyboard = ReplyKeyboardMarkup([
            [KeyboardButton("üìã Leads"), KeyboardButton("üíº Deals")],
            [KeyboardButton("üóìÔ∏è Activities")],
            [KeyboardButton("üß† Ask CAI"), KeyboardButton("üåê Change Language")]
        ], resize_keyboard=True)
        await message.reply("Welcome! Choose an option:", reply_markup=keyboard)
        user_data[user_id].clear()  # Clear user data on back
    elif any(f"{flag} {lang}" in text for lang, flag in languages.items()):
        selected_language = next((lang for lang, flag in languages.items() if f"{flag} {lang}" in text), "English")
        user_data[user_id]['language'] = selected_language
        await message.reply(f"Language changed to {selected_language}", reply_markup=ReplyKeyboardMarkup([[KeyboardButton("üîô Back")]], resize_keyboard=True))
    else:
        # Default handling for any message
        user_data[user_id]['context'].append(f"User's question: {text}")
        ai_response = await analyze_question_and_get_data(text, user_data[user_id]['context'], user_data[user_id]['language'])
        await message.reply(f"CAI Response:\n{ai_response}")
        user_data[user_id]['context'].append(f"CAI Response:\n{ai_response}")

# Run the bot
app.run()
